{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch Autograd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. PyTorch Basics for undersanding Autograds\n",
    "\n",
    "**Tensors**: It is just an n-dimensional array in PyTorch. Tensors support some additional enhancements which make them unique: \n",
    "* Apart from CPU, they can be loaded or the GPU for faster computations. \n",
    "* On setting `.requires_grad = True` they start forming a backward graph that tracks every operation applied on them to calculate the gradients using something called a dynamic computation graph (DCG).\n",
    "\n",
    "> In earlier versions of PyTorch, the `torch.autograd.Variable` class was used to create tensors that support gradient calculations and operation tracking but as of `PyTorch v0.4.0` Variable class has been deprecated. `torch.Tensor` and `torch.autograd.Variable` are now the same class. More precisely, `torch.Tensor` is capable of tracking history and behaves like the old Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], dtype=torch.float64, requires_grad=True)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(2, 2, requires_grad = True)\n",
    "\n",
    "# From numpy\n",
    "x = np.array([1., 2., 3.]) #Only Tensors of floating point dtype can require gradients\n",
    "x = torch.from_numpy(x)\n",
    "# Now enable gradient \n",
    "x.requires_grad_(True)\n",
    "# _ above makes the change in-place (its a common pytorch thing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Note: By PyTorch’s design, gradients can only be calculated for **floating** point tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Autograd**: This class is an engine to calculate derivatives (Jacobian-vector product to be more precise). \n",
    "* It records a graph of all the operations performed on a `gradient enabled` tensor and creates an acyclic graph called the dynamic computational graph. \n",
    "* The leaves of this graph are input tensors and the roots are output tensors. Gradients are calculated by tracing the graph from the root to the leaf and multiplying every gradient in the way using the chain rule."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Neural networks and Backpropagation\n",
    "\n",
    "Neural networks are nothing more than composite mathematical functions that are delicately tweaked (trained) to output the required result. The tweaking or the training is done through a remarkable algorithm called backpropagation. Backpropagation is used to calculate the gradients of the loss with respect to the input weights to later update the weights and eventually reduce the loss.\n",
    "\n",
    "> In a way, back propagation is just fancy name for the chain rule — Jeremy Howard\n",
    "\n",
    "Creating and training a neural network involves the following essential steps:\n",
    "1. Define the architecture\n",
    "2. Forward propagate on the architecture using input data\n",
    "3. Calculate the loss\n",
    "4. **Backpropagate to calculate the gradient for each weight**\n",
    "5. Update the weights using a learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The change in the loss for a small change in an input weight is called the gradient of that weight and is calculated using backpropagation. The gradient is then used to update the weight using a learning rate to overall reduce the loss and train the neural net.\n",
    "\n",
    "This is done in an iterative way. For each iteration, several gradients are calculated and something called a computation graph is built for storing these gradient functions. PyTorch does it by building a Dynamic Computational Graph (DCG). This graph is built from scratch in every iteration providing maximum flexibility to gradient calculation. For example, for a forward operation (function)`Mul` a backward operation (function) called `MulBackwardis` dynamically integrated in the backward graph for computing the gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Dynamic Computational graph\n",
    "\n",
    "Gradient enabled tensors (variables) along with functions (operations) combine to create the dynamic computational graph. \n",
    "* The flow of data and the operations applied to the data are defined at runtime hence constructing the computational graph dynamically. \n",
    "* This graph is made dynamically by the autograd class under the hood. [You don’t have to encode all possible paths before you launch the training — what you run is what you differentiate](https://pytorch.org/docs/stable/notes/autograd.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A simple DCG for multiplication of two tensors would look like this:\n",
    "\n",
    "<img src=\"./assets/simple_autograd.png\" width=\"430\" height=\"430\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every variable object has several attributes some important of which are:\n",
    "\n",
    "* **Data**: It is the data a variable is holding. `a` holds a 1x1 tensor with the value equal to 2.0 while `b` holds 3.0. `c` holds the product of two i.e. 6.0\n",
    "\n",
    "* **requires_grad**: This attribute, if true starts tracking all the operation history and forms a backward graph for gradient calculation. For an arbitrary tensor a It can be manipulated in-place as follows: `a.requires_grad_(True)`.\n",
    "\n",
    "> If there’s a single input to an operation that requires gradient, its output will also require gradient.\n",
    ">\n",
    "> Conversely, only if all inputs don’t require gradient, the output also won’t require it. \n",
    ">\n",
    "> Backward computation is never performed in the subgraphs, where all Tensors didn’t require gradients.\n",
    "\n",
    "* **grad**: grad holds the value of gradient. If `requires_grad` is False, it will hold a `None` value. Even if `requires_grad` is True, it will hold a `None` value unless `.backward()` function is called from some other node. For example, if you call `out.backward()` for some variable out that involved `x` in its calculations then `x.grad` will hold `∂out/∂x`.\n",
    "\n",
    "* **grad_fn**: This is the backward function used to calculate the gradient.\n",
    "\n",
    "* **is_leaf**: A node is leaf if :\n",
    "    * It was initialized explicitly by some function like `x = torch.tensor(1.0)` or `x = torch.randn(1, 1)` (basically all the tensor initializing methods).\n",
    "    * It is created after operations on tensors which all have `requires_grad = False`.\n",
    "    * It is created by calling `.detach()` method on some tensor.\n",
    "\n",
    "On calling `backward()`, gradients are populated only for the nodes which have both `requires_grad` and `is_leaf` True. Gradients are of the output node from which `.backward()` is called, w.r.t other leaf nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On turning `requires_grad = True` PyTorch will start tracking the operation and store the gradient functions at each step as follows:\n",
    "\n",
    "<img src=\"./assets/autograd_backprop.png\" width=\"550\" height=\"550\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the picture:\n",
    "\n",
    "* The `Mul` operational funcation has access to a context variable called `ctx` and it can store any values it needs for the backwards pass in `ctx`.\n",
    "* `ctx` would be passed to the `MulBackward` operation in the backward pass.\n",
    "* `MulBackward` function has the attribute `next_functions`, which is a list of tuples that each is associated with\n",
    "the different inputs that were passed to 'Mul' function.\n",
    "    * AccumulateGrad is associated with tensor `a` and it accumulates the gradient for the tensor `a`. \n",
    "    * None is associated with tensor `b`. It is none because tensor `b` has `requires_grad` set to `False` so we don't need to pass a gradient to it. \n",
    "\n",
    "The following code would generate the above graph under the PyTorch’s hood:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a\n",
      "data: 2.0\n",
      "requires_grad: True\n",
      "grad: None\n",
      "grad_fn: None\n",
      "is_leaf: True\n",
      "requires_grad: True\n",
      "b\n",
      "data: 3.0\n",
      "requires_grad: False\n",
      "grad: None\n",
      "grad_fn: None\n",
      "is_leaf: True\n",
      "requires_grad: False\n",
      "c\n",
      "data: 6.0\n",
      "requires_grad: True\n",
      "grad: None\n",
      "grad_fn: <MulBackward0 object at 0x11cba8cd0>\n",
      "is_leaf: False\n",
      "requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# Creating the graph\n",
    "a = torch.tensor(2.0, requires_grad = True)\n",
    "b = torch.tensor(3.0)\n",
    "c = a * b\n",
    "\n",
    "# Displaying\n",
    "for i, name in zip([a, b, c], \"abc\"):\n",
    "    print(f\"{name}\\ndata: {i.data}\\nrequires_grad: {i.requires_grad}\\n\\\n",
    "grad: {i.grad}\\ngrad_fn: {i.grad_fn}\\nis_leaf: {i.is_leaf}\\nrequires_grad: {i.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using **torch.no_grad() context** to turn off gradient calculation\n",
    "\n",
    "To stop PyTorch from tracking the history and forming the backward graph, the code can be wrapped inside the context `with torch.no_grad():`. It will make the code run faster whenever gradient tracking is not needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "# Creating the graph\n",
    "x = torch.tensor(1.0, requires_grad = True)\n",
    "# Check if tracking is enabled\n",
    "print(x.requires_grad) #True\n",
    "y = x * 2\n",
    "print(y.requires_grad) #True\n",
    "\n",
    "with torch.no_grad():\n",
    "    # Check if tracking is enabled\n",
    "    y = x * 2\n",
    "    print(y.requires_grad) #False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Backward() function\n",
    "\n",
    "Backward is the function which actually calculates the gradient by passing it’s argument (1x1 unit tensor by default) through the backward graph all the way up to every leaf node traceable from the calling root tensor. The calculated gradients are then stored in `.grad` of every **leaf node**. \n",
    "\n",
    "> Remember, the backward graph is already made dynamically during the forward pass. Backward function only calculates the gradient using the already made graph and stores them in leaf nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<PowBackward0 object at 0x104993790>\n",
      "tensor(3.)\n"
     ]
    }
   ],
   "source": [
    "# Creating the graph\n",
    "x = torch.tensor(1.0, requires_grad = True)\n",
    "z = x ** 3\n",
    "z.backward() #Computes the gradient \n",
    "print(z.grad_fn)\n",
    "print(x.grad.data) #Prints '3' which is dz/dx "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important thing to notice is that when `z.backward()` is called, a tensor is automatically passed as `z.backward(torch.tensor(1.0))`. The `torch.tensor(1.0)` is the external gradient provided to terminate the chain rule gradient multiplications. This external gradient is passed as the input to the `PowBackward` function to further calculate the gradient of `x`. The dimension of tensor passed into `.backward()` must be the same as the dimension of the tensor whose gradient is being calculated. \n",
    "\n",
    "For example, if the gradient enabled tensor `x` and `y` are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([0.0, 2.0, 8.0], requires_grad = True)\n",
    "y = torch.tensor([5.0 , 1.0 , 7.0], requires_grad = True)\n",
    "z = x * y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, to calculate gradients of `z` (a `1x3` tensor) with respect to `x` or `y` , an external gradient needs to be passed to `z.backward()`function as follows: `z.backward(torch.FloatTensor([1.0, 1.0, 1.0])`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<MulBackward0 object at 0x11cb9ab90>\n",
      "tensor([5., 1., 7.])\n"
     ]
    }
   ],
   "source": [
    "z.backward(torch.FloatTensor([1.0, 1.0, 1.0]))\n",
    "print(z.grad_fn)\n",
    "print(x.grad.data)       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> `z.backward()` would give a RuntimeError: grad can be implicitly created only for scalar outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This wouled give \"grad can be implicitly created only for scalar outputs\" error.\n",
    "# z.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The tensor passed into the backward function acts like weights for a weighted output of gradient. Mathematically, this is the vector multiplied by the Jacobian matrix of non-scalar tensors. Hence it should almost always be a unit tensor of dimension same as the tensor backward is called upon, unless weighted outputs needs to be calculated.\n",
    "\n",
    "> Backward graph is created automatically and dynamically by **autograd** class during **forward pass**. `Backward()` simply calculates the gradients by passing its argument to the already made backward graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Backward calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./assets/calculate_backprop.png\" width=\"580\" height=\"580\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward pass:\n",
      "a\n",
      "data: 2.0\n",
      "requires_grad: True\n",
      "grad: None\n",
      "grad_fn: None\n",
      "is_leaf: True\n",
      "requires_grad: True\n",
      "b\n",
      "data: 3.0\n",
      "requires_grad: False\n",
      "grad: None\n",
      "grad_fn: None\n",
      "is_leaf: True\n",
      "requires_grad: False\n",
      "c\n",
      "data: 6.0\n",
      "requires_grad: True\n",
      "grad: None\n",
      "grad_fn: <MulBackward0 object at 0x11cc22110>\n",
      "is_leaf: False\n",
      "requires_grad: True\n",
      "\n",
      "\n",
      "After backward on c:\n",
      "a\n",
      "data: 2.0\n",
      "requires_grad: True\n",
      "grad: 3.0\n",
      "grad_fn: None\n",
      "is_leaf: True\n",
      "requires_grad: True\n",
      "b\n",
      "data: 3.0\n",
      "requires_grad: False\n",
      "grad: None\n",
      "grad_fn: None\n",
      "is_leaf: True\n",
      "requires_grad: False\n",
      "c\n",
      "data: 6.0\n",
      "requires_grad: True\n",
      "grad: None\n",
      "grad_fn: <MulBackward0 object at 0x11cc225d0>\n",
      "is_leaf: False\n",
      "requires_grad: True\n"
     ]
    }
   ],
   "source": [
    "# Creating the graph\n",
    "a = torch.tensor(2.0, requires_grad = True)\n",
    "b = torch.tensor(3.0)\n",
    "c = a * b\n",
    "\n",
    "print(\"Forward pass:\")\n",
    "for i, name in zip([a, b, c], \"abc\"):\n",
    "    print(f\"{name}\\ndata: {i.data}\\nrequires_grad: {i.requires_grad}\\n\\\n",
    "grad: {i.grad}\\ngrad_fn: {i.grad_fn}\\nis_leaf: {i.is_leaf}\\nrequires_grad: {i.requires_grad}\")\n",
    "    \n",
    "c.backward()\n",
    "print(\"\\n\")\n",
    "print(\"After backward on c:\")\n",
    "# Displaying\n",
    "for i, name in zip([a, b, c], \"abc\"):\n",
    "    print(f\"{name}\\ndata: {i.data}\\nrequires_grad: {i.requires_grad}\\n\\\n",
    "grad: {i.grad}\\ngrad_fn: {i.grad_fn}\\nis_leaf: {i.is_leaf}\\nrequires_grad: {i.requires_grad}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Freeze specified layers\n",
    "\n",
    "Set **.requires_grad = False** of model's parameters is especially useful when you want to freeze part of your model, or you know in advance that you’re not going to use gradients w.r.t. some parameters. For example if you want to finetune a pretrained CNN, it’s enough to switch the `requires_grad` flags in the frozen base, and no intermediate buffers will be saved, until the computation gets to the last layer, where the affine transform will use weights that require gradient, and the output of the network will also require them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy feed-forward net\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(10, 5)\n",
    "        self.fc2 = nn.Linear(5, 5)\n",
    "        self.fc3 = nn.Linear(5, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc2 weight before train:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0712,  0.1818, -0.3325,  0.0632,  0.3794],\n",
      "        [ 0.4307, -0.3777, -0.0134, -0.3388,  0.3315],\n",
      "        [ 0.0073,  0.0054,  0.4047,  0.1378,  0.0897],\n",
      "        [ 0.3749, -0.0928, -0.0474, -0.0010,  0.2380],\n",
      "        [-0.2007, -0.2979, -0.2473, -0.0802,  0.0305]], requires_grad=True)\n",
      "fc2 weight after train:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0181,  0.1118, -0.3705,  0.0418,  0.4444],\n",
      "        [ 0.4631, -0.3315,  0.0101, -0.3224,  0.2941],\n",
      "        [-0.0406, -0.0575,  0.3705,  0.1188,  0.1484],\n",
      "        [ 0.3521, -0.1250, -0.0639, -0.0122,  0.2645],\n",
      "        [-0.2020, -0.2988, -0.2481, -0.0800,  0.0326]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# define random data\n",
    "random_input = torch.randn(10,)\n",
    "random_target = torch.randn(1,)\n",
    "\n",
    "# define net\n",
    "net = Net()\n",
    "\n",
    "# print fc2 weight\n",
    "print('fc2 weight before train:')\n",
    "print(net.fc2.weight)\n",
    "\n",
    "# train the net\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(net.parameters(), lr=0.1)\n",
    "for i in range(100):\n",
    "    net.zero_grad()\n",
    "    output = net(random_input)\n",
    "    loss = criterion(output, random_target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# print the trained fc2 weight\n",
    "print('fc2 weight after train:')\n",
    "print(net.fc2.weight)\n",
    "\n",
    "# save the net\n",
    "torch.save(net.state_dict(), 'model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc2 pretrained weight (same as the one above):\n",
      "Parameter containing:\n",
      "tensor([[ 0.0181,  0.1118, -0.3705,  0.0418,  0.4444],\n",
      "        [ 0.4631, -0.3315,  0.0101, -0.3224,  0.2941],\n",
      "        [-0.0406, -0.0575,  0.3705,  0.1188,  0.1484],\n",
      "        [ 0.3521, -0.1250, -0.0639, -0.0122,  0.2645],\n",
      "        [-0.2020, -0.2988, -0.2481, -0.0800,  0.0326]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# delete and redefine the net\n",
    "del net\n",
    "net = Net()\n",
    "\n",
    "# load the weight\n",
    "net.load_state_dict(torch.load('model'))\n",
    "\n",
    "# print the pre-trained fc2 weight\n",
    "print('fc2 pretrained weight (same as the one above):')\n",
    "print(net.fc2.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we want to freeze the fc2 layer this time: only train fc1 and fc3\n",
    "net.fc2.weight.requires_grad = False\n",
    "net.fc2.bias.requires_grad = False\n",
    "\n",
    "# NOTE: pytorch optimizer explicitly accepts parameter that requires grad\n",
    "# see https://github.com/pytorch/pytorch/issues/679\n",
    "# optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.1)\n",
    "# this raises ValueError: optimizing a parameter that doesn't require gradients\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc2 weight (frozen) after retrain:\n",
      "Parameter containing:\n",
      "tensor([[ 0.0181,  0.1118, -0.3705,  0.0418,  0.4444],\n",
      "        [ 0.4631, -0.3315,  0.0101, -0.3224,  0.2941],\n",
      "        [-0.0406, -0.0575,  0.3705,  0.1188,  0.1484],\n",
      "        [ 0.3521, -0.1250, -0.0639, -0.0122,  0.2645],\n",
      "        [-0.2020, -0.2988, -0.2481, -0.0800,  0.0326]])\n"
     ]
    }
   ],
   "source": [
    "# train again\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# define new random data\n",
    "random_input = torch.randn(10,)\n",
    "random_target = torch.randn(1,)\n",
    "\n",
    "for i in range(100):\n",
    "    net.zero_grad()\n",
    "    output = net(random_input)\n",
    "    loss = criterion(output, random_target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# print the retrained fc2 weight\n",
    "# note that the weight is same as the one before retraining: only fc1 & fc3 changed\n",
    "print('fc2 weight (frozen) after retrain:')\n",
    "print(net.fc2.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fc2 weight (unfrozen) after re-retrain:\n",
      "Parameter containing:\n",
      "tensor([[-0.2066,  0.2041, -0.4219,  0.1379,  0.5784],\n",
      "        [-0.1694,  0.4880, -0.8607,  0.2167,  0.0865],\n",
      "        [-0.4725,  0.1525,  0.1350,  0.3146,  0.1834],\n",
      "        [ 0.6044, -0.7760,  0.4370, -0.5776, -0.0451],\n",
      "        [-0.5125,  0.4816, -0.8304,  0.5518,  0.2252]], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "# let's unfreeze the fc2 layer this time for extra tuning\n",
    "net.fc2.weight.requires_grad = True\n",
    "net.fc2.bias.requires_grad = True\n",
    "\n",
    "# # add the unfrozen fc2 weight to the current optimizer\n",
    "# optimizer.add_param_group({'params': net.fc2.parameters()})\n",
    "\n",
    "# re-retrain\n",
    "for i in range(100):\n",
    "    net.zero_grad()\n",
    "    output = net(random_input)\n",
    "    loss = criterion(output, random_target)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "# print the re-retrained fc2 weight\n",
    "# note that this time the fc2 weight also changed\n",
    "print('fc2 weight (unfrozen) after re-retrain:')\n",
    "print(net.fc2.weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References:\n",
    "\n",
    "* [[Youtube] PyTorch Autograd Explained - In-depth Tutorial](https://www.youtube.com/watch?v=MswxJw-8PvE)\n",
    "* [Autograd Mechanics](https://pytorch.org/docs/stable/notes/autograd.html)\n",
    "* [PyTorch Autograd - Understanding the heart of PyTorch’s magic](https://towardsdatascience.com/pytorch-autograd-understanding-the-heart-of-pytorchs-magic-2686cd94ec95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
