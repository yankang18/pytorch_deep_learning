{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going Deeper with PyTorch Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post, we will be discussing how to implement more complex deep learning functionality using PyTorch. Some of the objectives of this posts are to make you understand.\n",
    "\n",
    "1. The difference between PyTorch classes like `nn.Module`, `nn.Functional`, `nn.Parameter` and when to use which.\n",
    "2. Custom training options such as different learning rates for different layers, different learning rate schedules\n",
    "3. Custom Weight Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import time\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Module vs nn.Functional"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyTorch, layers are often implemented as either one of `torch.nn.Module` objects or `torch.nn.Functional` functions. Which one to use?\n",
    "\n",
    "`torch.nn.Module` is basically the cornerstone of PyTorch. The way it works is you first define an `nn.Module` object,  and then invoke it's `forward` method to run it. This is a `Object Oriented` way of doing things.\n",
    "\n",
    "On the other hand, `nn.functional` provides some layers / activations in form of functions that can be directly called on the input rather than defining the an object. For example, in order to rescale an image tensor, you call `torch.nn.functional.interpolate` on an image tensor.\n",
    "\n",
    "So how do we choose which to use? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Stateful-ness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, any layer can be seen as a function. For example, a convolutional operation is just a bunch of multiplication and addition operations. So, it makes sense for us to just implement it as a function. However, the layer holds weights which need to be stored and updated while we are training. Therefore, from a programmatic angle, a layer is more than function. It also needs to hold data, which changes as we train our network.\n",
    "\n",
    "Data held by the convolutional layer **changes**. which means that the layer has a state which changes as we train. Either we can implement a function that does the convolutional operation, and define a data structure to hold the weights separately from the function itself, and then, make this external data structure an input to the function. Or\n",
    "\n",
    "> We can just define a class to hold the data structure, and make convolutional operation as an member function. This would really ease up our job, as we do not have to worry about stateful variables existing outside of the function. In these cases, we would prefer to use the `nn.Module` objects where we have weights or other states which might define the behaviour of the layer. For example, a dropout / Batch Norm layer behaves differently during training and inference.\n",
    "\n",
    "> On the other hand, where no state or weights are required, one could use the `nn.functional`. Examples being, resizing (`nn.functional.interpolate`),  average pooling (`nn.functional.AvgPool2d`).\n",
    "\n",
    "Despite the above reasoning, most of the `nn.Module` classes have their `nn.functional` counterparts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.Parameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important class in PyTorch is the `nn.Parameter` class. Consider the following case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[ 0.0252,  0.2836, -0.3039,  0.0171, -0.1406, -0.0774, -0.3154, -0.2542,\n",
      "         -0.2210, -0.0882],\n",
      "        [-0.2884,  0.2595,  0.1202, -0.2203,  0.2965, -0.1442, -0.2895, -0.3071,\n",
      "          0.2696,  0.2915],\n",
      "        [-0.2565, -0.2405,  0.1172, -0.2581, -0.0788, -0.2189,  0.0937, -0.1916,\n",
      "          0.2612, -0.0218],\n",
      "        [ 0.1434, -0.0104,  0.2019,  0.0474, -0.2460, -0.2612,  0.2012, -0.2539,\n",
      "         -0.0416,  0.1972],\n",
      "        [-0.1513, -0.1396,  0.0666,  0.2680,  0.1164,  0.1208, -0.0457, -0.2880,\n",
      "         -0.0297,  0.2010]], requires_grad=True), Parameter containing:\n",
      "tensor([ 0.0398,  0.1742,  0.1323, -0.1462,  0.1570], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "simple_net = SimpleNet()\n",
    "print(list(simple_net.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we define a net (subclass of `nn.Module`) and leverage a PyTorch built-in layer module such as `nn.Conv2d`, we do not need to add the parameters of `nn.Conv2d` to parameters of net. It happened implicitly by virtue of setting `nn.Conv2d` object as a member of the net object.\n",
    "\n",
    "This is internally facilitated by the `nn.Parameter` class, which subclasses the Tensor class. When we invoke `parameters()` function of a `nn.Module` object, it returns all its members which are `nn.Parameter` objects.\n",
    "\n",
    "> All the training weights of `nn.Module` classes are implemented as `nn.Parameter` objects. Whenever, a `nn.Module` object (nn.Conv2d in our case) is assigned as a member of another `nn.Module`, the \"parameters\" of the assignee `nn.Module` object (i.e. the weights of `nn.Conv2d`) are also added the \"parameters\" of the `nn.Module` object which is being assigned to (parameters of net object). This is called `registering` \"parameters\" of a `nn.Module`\n",
    "\n",
    "If you try to assign a tensor to the `nn.Module` object, it won't show up in the `parameters()` unless you define it as `nn.Parameter` object. This has been done to facilitate scenarios where you might need to cache a non-differentiable tensor, example in case, caching previous output in case of RNNs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('linear.weight', Parameter containing:\n",
      "tensor([[ 0.0852,  0.2504, -0.1351, -0.0247,  0.0808,  0.3004, -0.2279, -0.0225,\n",
      "          0.0478, -0.2327],\n",
      "        [-0.3057, -0.2100,  0.1080, -0.1771, -0.1293, -0.1323, -0.1504, -0.1108,\n",
      "         -0.1813, -0.3118],\n",
      "        [ 0.1201, -0.1547, -0.0886, -0.3014,  0.2451, -0.0148, -0.2312,  0.2971,\n",
      "          0.2368,  0.2860],\n",
      "        [-0.2337,  0.0546, -0.1255,  0.2235, -0.0570,  0.2856,  0.0514,  0.0917,\n",
      "          0.0964, -0.2239],\n",
      "        [ 0.1191, -0.0888, -0.1580, -0.0884,  0.2290,  0.3156, -0.1580, -0.2336,\n",
      "         -0.2232, -0.1244]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
      "tensor([ 0.0799,  0.0927, -0.1188, -0.0356,  0.0436], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "class net1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10,5)\n",
    "        self.tens = torch.ones(3,4) # This won't show up in a parameter list \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "myNet = net1()\n",
    "print(list(myNet.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('tens', Parameter containing:\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], requires_grad=True)), ('linear.weight', Parameter containing:\n",
      "tensor([[ 0.3014,  0.1304, -0.0963,  0.2978,  0.2582, -0.2182,  0.2930,  0.1501,\n",
      "         -0.1452,  0.2025],\n",
      "        [-0.1742, -0.2584, -0.2888, -0.2022, -0.2849, -0.3087, -0.0121,  0.0928,\n",
      "         -0.3006, -0.0834],\n",
      "        [ 0.1842, -0.1537, -0.2558,  0.0871,  0.1161, -0.2095, -0.0910,  0.0154,\n",
      "          0.3136, -0.1224],\n",
      "        [ 0.2583,  0.1148,  0.2160, -0.1627, -0.1739,  0.2727,  0.0292, -0.1146,\n",
      "          0.1640, -0.2952],\n",
      "        [ 0.2566,  0.1701, -0.1846,  0.0094, -0.0991,  0.2755, -0.0749,  0.2738,\n",
      "          0.0184,  0.2690]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
      "tensor([-0.2042,  0.2643, -0.2221, -0.0116, -0.2130], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "class net2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10,5) \n",
    "        self.tens = nn.Parameter(torch.ones(3,4)) # This will show up in a parameter list \n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "myNet = net2()\n",
    "print(list(myNet.named_parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('linear.weight', Parameter containing:\n",
      "tensor([[ 0.0612,  0.2361,  0.2638,  0.1153,  0.1726,  0.3016,  0.1860, -0.1373,\n",
      "         -0.1924,  0.2928],\n",
      "        [ 0.0947, -0.2412, -0.2797, -0.2670, -0.0784,  0.0892,  0.2124, -0.3056,\n",
      "          0.1527, -0.1618],\n",
      "        [-0.1180,  0.2467,  0.0416,  0.1575,  0.1210,  0.2373, -0.3120, -0.1024,\n",
      "         -0.0893, -0.0759],\n",
      "        [-0.2030,  0.1476,  0.0172, -0.1674,  0.0695, -0.2864,  0.0680, -0.0952,\n",
      "          0.2752,  0.1475],\n",
      "        [ 0.1653,  0.1993,  0.1463,  0.1912,  0.2653, -0.2367, -0.2035, -0.2700,\n",
      "          0.2169,  0.0812]], requires_grad=True)), ('linear.bias', Parameter containing:\n",
      "tensor([ 0.2582, -0.0540, -0.3131, -0.1285,  0.1488], requires_grad=True)), ('sub_net.tens', Parameter containing:\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]], requires_grad=True)), ('sub_net.linear.weight', Parameter containing:\n",
      "tensor([[-0.2313,  0.1468, -0.0252, -0.3159,  0.3098,  0.1960,  0.0514,  0.0687,\n",
      "          0.1075, -0.2739],\n",
      "        [-0.1181,  0.2779,  0.1258, -0.0815,  0.1121,  0.2707,  0.2535,  0.2065,\n",
      "         -0.1869, -0.2368],\n",
      "        [-0.2686,  0.1162, -0.1033, -0.2002, -0.2808, -0.2290, -0.0441, -0.1731,\n",
      "          0.0878,  0.2913],\n",
      "        [-0.2960, -0.3133, -0.2327, -0.1175, -0.0493, -0.2436,  0.2927, -0.1167,\n",
      "          0.1821, -0.0679],\n",
      "        [-0.1138, -0.2943, -0.0953, -0.0516, -0.0615, -0.2163,  0.2774,  0.0164,\n",
      "         -0.2133,  0.2974]], requires_grad=True)), ('sub_net.linear.bias', Parameter containing:\n",
      "tensor([0.0479, 0.0053, 0.2992, 0.1901, 0.0486], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "class net3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(10,5) \n",
    "        self.sub_net  = net2() # Parameters of net2 will show up in list of parameters of net3\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "myNet = net3()\n",
    "print(list(myNet.named_parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## nn.ModuleList and nn.ParameterList()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we made a `nn.ModuleList` object a member of our defined net class, all members, including their parameters, inside the `nn.ModuleList` list will be registered as a member of the net object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('layers.0.weight', Parameter containing:\n",
      "tensor([[[[ 0.1992,  0.1196, -0.0622],\n",
      "          [-0.0323,  0.0642,  0.0576],\n",
      "          [ 0.0334, -0.1276, -0.0024]],\n",
      "\n",
      "         [[-0.2122, -0.0962, -0.1099],\n",
      "          [-0.1497, -0.1558, -0.1632],\n",
      "          [-0.2349,  0.2146, -0.0615]]],\n",
      "\n",
      "\n",
      "        [[[ 0.0285, -0.0818,  0.0092],\n",
      "          [-0.2058, -0.2257,  0.1678],\n",
      "          [-0.0912, -0.1038, -0.1138]],\n",
      "\n",
      "         [[ 0.0257, -0.2277,  0.0759],\n",
      "          [ 0.0907,  0.1693, -0.0356],\n",
      "          [-0.1944, -0.1999,  0.0489]]]], requires_grad=True)), ('layers.0.bias', Parameter containing:\n",
      "tensor([0.1099, 0.0754], requires_grad=True)), ('layers.1.weight', Parameter containing:\n",
      "tensor([1., 1., 1., 1., 1.], requires_grad=True)), ('layers.1.bias', Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0.], requires_grad=True)), ('layers.2.weight', Parameter containing:\n",
      "tensor([[ 0.3045, -0.0475,  0.0247,  0.3987,  0.2710],\n",
      "        [ 0.3516, -0.2260, -0.3656,  0.0198, -0.0681]], requires_grad=True)), ('layers.2.bias', Parameter containing:\n",
      "tensor([-0.3858,  0.3461], requires_grad=True))]\n"
     ]
    }
   ],
   "source": [
    "layer_list = [nn.Conv2d(2,2,3), nn.BatchNorm2d(5), nn.Linear(5,2)]\n",
    "\n",
    "class myNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList(layer_list)\n",
    "  \n",
    "    def forward(x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "net = myNet()\n",
    "\n",
    "print(list(net.named_parameters()))  # Parameters of modules in layer_list show up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you see, assigning a Python List doesn't register the parameters of Modules inside the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[]\n"
     ]
    }
   ],
   "source": [
    "layer_list = [nn.Conv2d(5,5,3), nn.BatchNorm2d(5), nn.Linear(5,2)]\n",
    "\n",
    "class myNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layers = layer_list\n",
    "  \n",
    "    def forward(x):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "\n",
    "net = myNet()\n",
    "\n",
    "print(list(net.parameters()))  # Parameters of modules in the layer_list don't show up."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight initialization can influence the results of training. We may require different weight initialization schemes for different sort of layers. This can be accomplished by the `modules()` function, which is a member function of the `nn.Module` class that returns an iterator containing all the `nn.Module` member objects of that `nn.Module` class. \n",
    "\n",
    "There are a plethora of inplace initialization functions to be found in the `torch.nn.init` module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNet2(\n",
      "  (conv): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (bn): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "Parameter containing:\n",
      "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0.], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAD7CAYAAACRxdTpAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjMsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+AADFEAAAP1ElEQVR4nO3dfYxldX3H8fenQO2DJmB3oBtgO2pWIxpddUJpiIaKbRcwIiZYSEOpko4mkGjiH66aFGtiQqtoYmwxa9iACUVpECEFq1uCEpOiLrpdly7oQlcZ2eyO0AoGQ7PLt3/MmfQ63GXu3Aeu++P9Sm7uOd/z9D0BPpw59zykqpAkteU3pt2AJGn8DHdJapDhLkkNMtwlqUGGuyQ1yHCXpAatGu5JTk1yV5I9Se5L8r6u/uIk25P8qPs+oasnyWeS7E2yK8nrJ70TkqRfNciR+yHgA1X1SuAM4PIkpwFbgDuraiNwZzcOcA6wsfvMA9eMvWtJ0rM6drUZqmo/sL8bfiLJHuBk4HzgrG6264FvAB/s6l+opbuj7klyfJL13Xr6WrduXc3Ozo6wG5L0/HPvvff+rKpm+k1bNdx7JZkFXgd8GzhpObCran+SE7vZTgYe7llsoasdMdxnZ2fZsWPHWlqRpOe9JD8+0rSBf1BN8kLgZuD9VfX4s83ap/aMZxwkmU+yI8mOxcXFQduQJA1goHBPchxLwX5DVX25Kx9Isr6bvh442NUXgFN7Fj8FeGTlOqtqa1XNVdXczEzfvyokSUMa5GqZANcCe6rqUz2TbgMu7YYvBW7tqf9ld9XMGcDPn+18uyRp/AY5534mcAnwgyQ7u9qHgauAm5JcBvwEuLCbdgdwLrAXeBJ411g7liStapCrZb5F//PoAGf3mb+Ay0fsS5I0Au9QlaQGGe6S1CDDXZIaZLhLUoPWdIeqNC2zW26f2rb3XXXe1LYtDcsjd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYN8oLsbUkOJtndU/tSkp3dZ9/yu1WTzCb5Zc+0z02yeUlSf4M88vc64LPAF5YLVfXny8NJrgZ+3jP/g1W1aVwNSpLWbpAXZN+dZLbftCQB3gm8ebxtSZJGMeo59zcCB6rqRz21lyT5fpJvJnnjiOuXJA1h1DcxXQzc2DO+H9hQVY8meQPwlSSvqqrHVy6YZB6YB9iwYcOIbUiSeg0d7kmOBd4BvGG5VlVPAU91w/cmeRB4ObBj5fJVtRXYCjA3N1fD9iFN2rRe8efr/TSKUU7LvAW4v6oWlgtJZpIc0w2/FNgIPDRai5KktVr1yD3JjcBZwLokC8CVVXUtcBG/ekoG4E3Ax5IcAg4D762qx8bbsqZpmi+qljS4Qa6WufgI9b/qU7sZuHn0tiRJo/AOVUlqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDVo13JNsS3Iwye6e2keT/DTJzu5zbs+0DyXZm+SBJH82qcYlSUc2yJH7dcDmPvVPV9Wm7nMHQJLTWHpx9qu6Zf4xyTHjalaSNJhVw72q7gYeG3B95wNfrKqnquq/gL3A6SP0J0kawijn3K9Isqs7bXNCVzsZeLhnnoWuJkl6Dg0b7tcALwM2AfuBq7t6+sxb/VaQZD7JjiQ7FhcXh2xDktTPUOFeVQeq6nBVPQ18nv8/9bIAnNoz6ynAI0dYx9aqmququZmZmWHakCQdwVDhnmR9z+gFwPKVNLcBFyV5QZKXABuB74zWoiRprY5dbYYkNwJnAeuSLABXAmcl2cTSKZd9wHsAquq+JDcB/wkcAi6vqsOTaV2SdCSrhntVXdynfO2zzP9x4OOjNCVJGo13qEpSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJatCq4Z5kW5KDSXb31D6R5P4ku5LckuT4rj6b5JdJdnafz02yeUlSf4McuV8HbF5R2w68uqpeA/wQ+FDPtAeralP3ee942pQkrcWq4V5VdwOPrah9vaoOdaP3AKdMoDdJ0pDGcc793cBXe8ZfkuT7Sb6Z5I1jWL8kaY2OHWXhJB8BDgE3dKX9wIaqejTJG4CvJHlVVT3eZ9l5YB5gw4YNo7QhSVph6CP3JJcCbwX+oqoKoKqeqqpHu+F7gQeBl/dbvqq2VtVcVc3NzMwM24YkqY+hwj3JZuCDwNuq6sme+kySY7rhlwIbgYfG0agkaXCrnpZJciNwFrAuyQJwJUtXx7wA2J4E4J7uypg3AR9Lcgg4DLy3qh7ru2JJ0sSsGu5VdXGf8rVHmPdm4OZRm5IkjcY7VCWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNWigcE+yLcnBJLt7ai9Osj3Jj7rvE7p6knwmyd4ku5K8flLNS5L6G/TI/Tpg84raFuDOqtoI3NmNA5wDbOw+88A1o7cpSVqLgcK9qu4GHltRPh+4vhu+Hnh7T/0LteQe4Pgk68fRrCRpMKOccz+pqvYDdN8ndvWTgYd75lvoapKk58gkflBNn1o9Y6ZkPsmOJDsWFxcn0IYkPX+NEu4Hlk+3dN8Hu/oCcGrPfKcAj6xcuKq2VtVcVc3NzMyM0IYkaaVjR1j2NuBS4Kru+9ae+hVJvgj8IfDz5dM3Go/ZLbdPuwVJv+YGCvckNwJnAeuSLABXshTqNyW5DPgJcGE3+x3AucBe4EngXWPuWZK0ioHCvaouPsKks/vMW8DlozQlabp/oe276rypbVvj4R2qktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaNPQLspO8AvhST+mlwN8AxwN/DSx29Q9X1R1DdyhJWrOhw72qHgA2ASQ5BvgpcAtLL8T+dFV9ciwdSpLWbFynZc4GHqyqH49pfZKkEYwr3C8CbuwZvyLJriTbkpwwpm1IkgY0crgn+U3gbcA/d6VrgJexdMpmP3D1EZabT7IjyY7FxcV+s0iShjSOI/dzgO9V1QGAqjpQVYer6mng88Dp/Raqqq1VNVdVczMzM2NoQ5K0bBzhfjE9p2SSrO+ZdgGwewzbkCStwdBXywAk+R3gT4D39JT/PskmoIB9K6ZJkp4DI4V7VT0J/N6K2iUjdSRJGpl3qEpSgwx3SWqQ4S5JDTLcJalBhrskNchwl6QGGe6S1CDDXZIaNNJNTJLaNLvl9qlsd99V501luy3yyF2SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAZ5nfsIpnUtsCStxiN3SWrQyEfuSfYBTwCHgUNVNZfkxcCXgFmW3qP6zqr671G3JUkazLiO3P+4qjZV1Vw3vgW4s6o2And245Kk58ikTsucD1zfDV8PvH1C25Ek9TGOcC/g60nuTTLf1U6qqv0A3feJY9iOJGlA47ha5syqeiTJicD2JPcPslD3P4J5gA0bNoyhDUnSspGP3Kvqke77IHALcDpwIMl6gO77YJ/ltlbVXFXNzczMjNqGJKnHSOGe5HeTvGh5GPhTYDdwG3BpN9ulwK2jbEeStDajnpY5CbglyfK6/qmq/jXJd4GbklwG/AS4cMTtSJLWYKRwr6qHgNf2qT8KnD3KuiVJw/MOVUlqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSgwx3SWqQ4S5JDRo63JOcmuSuJHuS3JfkfV39o0l+mmRn9zl3fO1KkgYxyjtUDwEfqKrvJXkRcG+S7d20T1fVJ0dvT5I0jKHDvar2A/u74SeS7AFOHldjazG75fZpbFaSfm2N5Zx7klngdcC3u9IVSXYl2ZbkhHFsQ5I0uJHDPckLgZuB91fV48A1wMuATSwd2V99hOXmk+xIsmNxcXHUNiRJPUYK9yTHsRTsN1TVlwGq6kBVHa6qp4HPA6f3W7aqtlbVXFXNzczMjNKGJGmFUa6WCXAtsKeqPtVTX98z2wXA7uHbkyQNY5SrZc4ELgF+kGRnV/swcHGSTUAB+4D3jNShJGnNRrla5ltA+ky6Y/h2JEnj4B2qktSgUU7LSNJYTeuelX1XnTeV7U6SR+6S1CDDXZIaZLhLUoMMd0lqkOEuSQ0y3CWpQYa7JDXIcJekBhnuktQgw12SGmS4S1KDDHdJapDhLkkNMtwlqUGGuyQ1yOe5S3rem9Zz5GFyz5Kf2JF7ks1JHkiyN8mWSW1HkvRMEwn3JMcA/wCcA5zG0kuzT5vEtiRJzzSpI/fTgb1V9VBV/S/wReD8CW1LkrTCpML9ZODhnvGFriZJeg5M6gfV9KnVr8yQzAPz3egvkjwwoV7GZR3ws2k3MWHu49Gv9f2DxvYxf9e3POg+/sGRJkwq3BeAU3vGTwEe6Z2hqrYCWye0/bFLsqOq5qbdxyS5j0e/1vcP3MdBTeq0zHeBjUlekuQ3gYuA2ya0LUnSChM5cq+qQ0muAL4GHANsq6r7JrEtSdIzTewmpqq6A7hjUuufgqPmFNII3MejX+v7B+7jQFJVq88lSTqq+GwZSWqQ4T6gJJ9Icn+SXUluSXL8tHsatyQXJrkvydNJmroaofXHYSTZluRgkt3T7mVSkpya5K4ke7p/T9837Z7GLclvJflOkv/o9vFvh12X4T647cCrq+o1wA+BD025n0nYDbwDuHvajYzT8+RxGNcBm6fdxIQdAj5QVa8EzgAub/Cf41PAm6vqtcAmYHOSM4ZZkeE+oKr6elUd6kbvYena/aZU1Z6q+nW/mWwYzT8Oo6ruBh6bdh+TVFX7q+p73fATwB4au/O9lvyiGz2u+wz1w6jhPpx3A1+ddhMamI/DaEySWeB1wLen28n4JTkmyU7gILC9qobaR5/n3iPJvwG/32fSR6rq1m6ej7D05+ENz2Vv4zLIPjZo1cdh6OiR5IXAzcD7q+rxafczblV1GNjU/a53S5JXV9Waf0sx3HtU1VuebXqSS4G3AmfXUXoN6Wr72KhVH4eho0OS41gK9huq6svT7meSqup/knyDpd9S1hzunpYZUJLNwAeBt1XVk9PuR2vi4zAakCTAtcCeqvrUtPuZhCQzy1fiJflt4C3A/cOsy3Af3GeBFwHbk+xM8rlpNzRuSS5IsgD8EXB7kq9Nu6dx6H4IX34cxh7gptYeh5HkRuDfgVckWUhy2bR7moAzgUuAN3f/De5Mcu60mxqz9cBdSXaxdFCyvar+ZZgVeYeqJDXII3dJapDhLkkNMtwlqUGGuyQ1yHCXpAYZ7pLUIMNdkhpkuEtSg/4P0kEs6rv5pXMAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "class SimpleNet2(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(10, 10, 3)\n",
    "        self.bn = nn.BatchNorm2d(10)\n",
    "        \n",
    "    def weights_init(self):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv2d):\n",
    "                nn.init.normal_(module.weight, mean=0, std=1)\n",
    "                nn.init.constant_(module.bias, 0)\n",
    "                \n",
    "simple_net2 = SimpleNet2()\n",
    "simple_net2.weights_init()\n",
    "print(simple_net2)\n",
    "\n",
    "for module in simple_net2.modules():\n",
    "    if isinstance(module, nn.Conv2d):\n",
    "        weights = module.weight\n",
    "        weights = weights.reshape(-1).detach().numpy()\n",
    "        print(module.bias)\n",
    "        plt.hist(weights)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## modules() vs children()\n",
    "\n",
    "A very similar function to modules is children. The difference is a slight but an important one. As we know, a nn.Module object can contain other nn.Module objects as it's data members.\n",
    "\n",
    "* `children()` will only return a list of the `nn.Module` objects which are members of the object on which children is being called.\n",
    "\n",
    "* `modules()` goes **recursively** inside each `nn.Module` object, creating a list of each `nn.Module` object that comes along the way until there are no `nn.module` objects left. Note that `modules()` also returns the `nn.Module` object on which the `modules()` has been called as a part of the list.\n",
    "\n",
    "> Therefore, when we initialize the weights, we might want to use `modules()` function since we can't go inside the `nn.Sequential` object and initialize the weight for its members.\n",
    "\n",
    "Note, that the above statement remains true for subclasses inherited from `nn.Module` class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Printing children\n",
      "------------------------------\n",
      "[Sequential(\n",
      "  (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "), Linear(in_features=10, out_features=2, bias=True)]\n",
      "\n",
      "\n",
      "Printing Modules\n",
      "------------------------------\n",
      "0 :myNet(\n",
      "  (convBN): Sequential(\n",
      "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (linear): Linear(in_features=10, out_features=2, bias=True)\n",
      ")\n",
      "1 :Sequential(\n",
      "  (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ")\n",
      "2 :Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "3 :BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "4 :Linear(in_features=10, out_features=2, bias=True)\n"
     ]
    }
   ],
   "source": [
    "class myNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.convBN =  nn.Sequential(nn.Conv2d(10,10,3), nn.BatchNorm2d(10))\n",
    "        self.linear =  nn.Linear(10,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        pass\n",
    "\n",
    "net = myNet()\n",
    "\n",
    "print(\"Printing children\\n------------------------------\")\n",
    "print(list(net.children()))\n",
    "print(\"\\n\\nPrinting Modules\\n------------------------------\")\n",
    "# print(list(net.modules()))\n",
    "for idx, m in enumerate(net.modules()):\n",
    "    print(\"{0} :{1}\".format(idx, m))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Printing Information About the Network\n",
    "We may need to print information about the network, whether be it for the user or for debugging purposes. PyTorch provides a neat way to print a lot of information about out network using it's `named_*` functions. There are 4 such functions.\n",
    "\n",
    "1. `named_parameters`, returns an iterator which gives a tuple containing **name** of the parameters (if a convolutional layer is assigned as `self.conv1`, then it's parameters would be `conv1.weight` and `conv1.bias`) and the value returned by the **__repr__** function of the `nn.Parameter`.\n",
    "2. `named_modules`. Same as above, but iterator returns modules like `modules()` function does.\n",
    "\n",
    "3. `named_children` Same as above, but iterator return modules like `children()` returns.\n",
    "\n",
    "4. `named_buffers` Return buffer tensors such as running mean average of a Batch Norm layer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " , myNet(\n",
      "  (convBN): Sequential(\n",
      "    (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "    (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  )\n",
      "  (linear): Linear(in_features=10, out_features=2, bias=True)\n",
      ") \n",
      "-------------------------------\n",
      "convBN , Sequential(\n",
      "  (0): Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1))\n",
      "  (1): BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      ") \n",
      "-------------------------------\n",
      "convBN.0 , Conv2d(10, 10, kernel_size=(3, 3), stride=(1, 1)) \n",
      "-------------------------------\n",
      "convBN.1 , BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True) \n",
      "-------------------------------\n",
      "linear , Linear(in_features=10, out_features=2, bias=True) \n",
      "-------------------------------\n"
     ]
    }
   ],
   "source": [
    "for x in net.named_modules():\n",
    "    print(x[0], \",\" ,x[1], \"\\n-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Different Learning Rates For Different Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we will learn how to use different learning rates for our different layers. In general, we will cover how to have different hyperparameters for different groups of parameters, whether it be different learning rate for different layers, or different learning rate for biases and weights.\n",
    "\n",
    "n our previous post, where we implemented a CIFAR classifier, we passed all the parameters of network as a whole to the optimiser object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class myNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10,5)\n",
    "        self.fc2 = nn.Linear(5,2)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.fc1(x))\n",
    "\n",
    "net = myNet()\n",
    "optimizer = torch.optim.SGD(net.parameters(), lr = 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the `torch.optim` class allows us to provide different sets of parameters with different learning rates in form of a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SGD (\n",
       "Parameter Group 0\n",
       "    dampening: 0\n",
       "    lr: 0.001\n",
       "    momentum: 0.99\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       "\n",
       "Parameter Group 1\n",
       "    dampening: 0\n",
       "    lr: 0.01\n",
       "    momentum: 0\n",
       "    nesterov: False\n",
       "    weight_decay: 0\n",
       ")"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "optimizer = torch.optim.SGD([{\"params\": net.fc1.parameters(), \"lr\" : 0.001, \"momentum\" : 0.99},\n",
    "                             {\"params\": net.fc2.parameters(), \"lr\" : 0.01}])\n",
    "\n",
    "optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above scenario, the parameters of `fc1` use a learning rate of 0.001 and momentum of 0.99. If a hyperparameter is not specified for a group of parameters (like `fc2`), they use the default value of that hyperparameter. You could create parameter lists on basis of different layers, or layers' weights or biases, using the `named_parameters()` function we covered above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Rate Scheduling\n",
    "\n",
    "Learning rate is a major hyperparameter that we want to tune. PyTorch provides support for scheduling learning rates with it's `torch.optim.lr_scheduler` module which has a variety of learning rate schedules. The following example demonstrates one such example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones = [10,20], gamma = 0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `MultiStepLR` scheduler multiplies the learning rate by `gamma` each time when we reach `epochs` contained in the `milestones` list. In our example, the learning rate is multiplied by 0.1 at the 10nth and the 20nth epoch. You will also have to write the line `scheduler.step` in the loop in your code that goes over the epochs so that the learning rate is updated.\n",
    "\n",
    "> Generally, training loop is made of two nested loops, where one loop goes over the epochs, and the nested one goes over the batches in that epoch. Make sure you call `scheduler.step` at start of the epoch loop so your learning rate is updated. Be careful not to write it in the batch loop, otherwise your learning rate may be updated at the 10th batch rather than 10nth epoch.\n",
    "\n",
    "> Also remember that `scheduler.step` is no replacement for `optim.step` and we will have to call `optim.step` everytime we backprop backwards. (This would be in the \"batch\" loop)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
