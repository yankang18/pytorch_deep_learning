{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom Dataset and Data Loader\n",
    "\n",
    "This document is basically copied from [pytorch-101-building-neural-networks](https://blog.paperspace.com/pytorch-101-building-neural-networks/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "from PIL import Image\n",
    "import random\n",
    "import time\n",
    "import torchvision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normally, there are two classes PyTorch provides you in relation to build input pipelines to load data.\n",
    "\n",
    "1. `torch.data.utils.dataset`, which we will just refer as the dataset class now.\n",
    "2. `torch.data.utils.dataloader` , which we will just refer as the dataloader class now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`dataset` is a class that loads the data and returns a generator so that you iterate over it. It also lets you incorporate data augmentation techniques into the input Pipeline.\n",
    "\n",
    "If you want to create a `dataset` object for your data, you need to overload three functions.\n",
    "\n",
    "1. `__init__` function. Here, you define things related to your dataset here. Most importantly, the location of your data. You can also define various data augmentations you want to apply.\n",
    "2. `__len__` function. Here, you just return the length of the dataset.\n",
    "3. `__getitem__` function. The function takes as an argument an index `i` and returns a data example. This function would be called every iteration during our training loop with a different `i` by the `dataset` object.\n",
    "\n",
    "Here is a implementation of our `dataset` object for the CIFAR dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Cifar10Dataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, data_dir, data_size, transforms=None):\n",
    "        files = os.listdir(data_dir)\n",
    "        files = [os.path.join(data_dir, x) for x in files]\n",
    "        \n",
    "        if data_size < 0 or data_size > len(files):\n",
    "            raise Exception(\"Data size should be between 0 and {0}\".format(len(files)))\n",
    "            \n",
    "        if data_size == 0:\n",
    "            data_size = len(files)\n",
    "            \n",
    "        self.data_size = data_size\n",
    "        self.files = random.sample(files, self.data_size)\n",
    "        self.transforms = transforms\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.data_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        image_address = self.files[index]\n",
    "        image = Image.open(image_address)\n",
    "        image = preprocess(image)\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "opencv",
   "language": "python",
   "name": "opencv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
